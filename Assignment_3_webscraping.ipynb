{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    driver.get(\"https://www.naukri.com/data-scientist-jobs-in-delhi?k=data%20scientist&1=delhi\")\n",
    "    delay = 5                 # seconds\n",
    "    WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.ID,'chk-0-3 Lakhs-ctcFilter-')))\n",
    "    # Above code will wait for the presence of the given element for a max of 5 seconds\n",
    "    salary=driver.find_element_by_id(\"chk-0-3 Lakhs-ctcFilter-\")\n",
    "    time.sleep(3)\n",
    "    #driver.get(driver.current_url)\n",
    "    #salary.click()\n",
    "except  NoSuchElementException as e:\n",
    "    print(\"Exception raised: \", e)\n",
    "except StaleElementReferenceException as e:\n",
    "    print(\"Exception raised: \", e)\n",
    "else:\n",
    "    print('Done')\n",
    "finally:\n",
    "    driver.close()\n",
    "    print('Finally driver closed')\n",
    "print('Last Statement')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3928ccc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaa82ff0",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q. 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import selenium as sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = sl.webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key = 'led tv 50 + inches full hd smart tv'  \n",
    "key = input('Enter any item to search  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27f2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.amazon.in\")\n",
    "time.sleep(2)\n",
    "s_input = driver.find_element_by_xpath('//*[@id=\"twotabsearchtextbox\"]')\n",
    "s_input.send_keys(key)\n",
    "s_btn= driver.find_element_by_xpath('//*[@id=\"nav-search-submit-button\"]')\n",
    "s_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48205406",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q. 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb56d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "rng = range(3)\n",
    "maxdelay = 10\n",
    "for i in rng:\n",
    "    print(\"  loop count:\", i,\"  Page:\", driver.current_url )\n",
    "    #lastpg = driver.current_url\n",
    "    br_nm_tags = driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-link-style a-text-normal']\")\n",
    "    if(len(br_nm_tags)<1):\n",
    "        print('  More items not found exiting due to br_nm_tags len = 0')\n",
    "        break\n",
    "    for j in br_nm_tags:\n",
    "        links.append(j.get_attribute('href'))\n",
    "    try:\n",
    "        #btn = driver.find_element_by_xpath('//span[@class=\"s-pagination-strip\"]/a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')\n",
    "        btn = driver.find_element_by_xpath('//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')\n",
    "        btn.click()\n",
    "        WebDriverWait(driver, maxdelay).until(EC.presence_of_element_located((By.CLASS_NAME,'s-pagination-strip')))\n",
    "    except:\n",
    "        print('  More items not found exiting due to exception')\n",
    "        break\n",
    "print(\"Total no. of Links found :\", len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30df6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = []\n",
    "product = []\n",
    "mrp = []\n",
    "dp = []\n",
    "er = []\n",
    "dlvr = []\n",
    "avl = []\n",
    "url = []\n",
    "bcount=0\n",
    "for i in links:\n",
    "    bcount+=1\n",
    "    if bcount > 6:\n",
    "        break\n",
    "        \n",
    "    driver.get(i)\n",
    "    WebDriverWait(driver, maxdelay).until(EC.presence_of_element_located((By.ID,'productOverview_feature_div')))\n",
    "    try:\n",
    "        brand.append( driver.find_element_by_xpath('//div[@id=\"productOverview_feature_div\"]/div/div[1]/div/table/tbody/tr[4]/td[2]/span').text)\n",
    "    except:\n",
    "        brand.append('-')\n",
    "        \n",
    "    try:\n",
    "        product.append(driver.find_element_by_xpath('//span[@id=\"productTitle\"]').text)\n",
    "    except:\n",
    "        product.append('-')\n",
    "        \n",
    "    try:\n",
    "        mrp.append(driver.find_element_by_xpath('//span[@class=\"a-price a-text-price a-size-base\"]/span[1]').get_attribute('innerHTML'))\n",
    "    except:\n",
    "        mrp.append('-')\n",
    "        \n",
    "    try:\n",
    "        dp.append(driver.find_element_by_xpath('//span[@class=\"a-price a-text-price a-size-medium apexPriceToPay\"]/span[1]').get_attribute('innerHTML'))\n",
    "    except:\n",
    "        dp.append('-')\n",
    "        \n",
    "    try:\n",
    "        er.append(driver.find_element_by_xpath('//div[@id=\"RETURNS_POLICY\"]').text)\n",
    "    except:\n",
    "        er.append('-')\n",
    "        \n",
    "    try:\n",
    "        dlvr.append(driver.find_element_by_xpath('//span[@id=\"freeShippingPriceBadging_feature_div\"]/span').text.replace('Fulfilled',''))\n",
    "    except:\n",
    "        dlvr.append('-')\n",
    "        \n",
    "    try:\n",
    "        avl.append(driver.find_element_by_xpath('//div[@id=\"availability\"]').text)\n",
    "    except:\n",
    "        avl.append('-')\n",
    "        \n",
    "    url.append(i)\n",
    "    #if driver.find_element_by_xpath('//div[@id=\"feature-bullets\"]/ul').text.lower().find('easy return'):\n",
    "    #    er.append('Yes')\n",
    "    #else:\n",
    "    #    er.append('No')\n",
    "    #break\n",
    "df = pd.DataFrame()\n",
    "df['Product']= product\n",
    "df['Brand']=brand\n",
    "df['MRP']=mrp\n",
    "df['DealP']= dp\n",
    "df['EasyReturn'] = er\n",
    "df['Delivery'] = dlvr\n",
    "df['Availability']=avl\n",
    "df['URL']= url\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e053016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\temp\\tvs_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0f30f",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6132e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best prog working ok\n",
    "def Download_Images(search_input, no_of_figs):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"en-US,en;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "\n",
    "    url = 'https://images.google.com'\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    srch_bar = driver.find_element_by_xpath('//input')\n",
    "    srch_bar.clear\n",
    "    btn = driver.find_elements_by_xpath('//button')[1]\n",
    "    srch_bar.send_keys(search_input)\n",
    "    btn.click()\n",
    "    \n",
    "#     delay = 5\n",
    "#     WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME,'rg_i')))\n",
    "    time.sleep(3)\n",
    "    img_tags = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "    \n",
    "    count=0\n",
    "    for img_tag in img_tags:\n",
    "        if count> no_of_figs-1:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            img_tag.click()\n",
    "        except Exception as ex:\n",
    "            template = \"    img_tag.click: An exception of type {0} occurred.\" # Arguments:\\n{1!r}\"\n",
    "            message = template.format(type(ex).__name__) #, ex.args)\n",
    "            print(message)\n",
    "            continue\n",
    "        \n",
    "#         delay =10\n",
    "#         WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME,'n3VNCb')))\n",
    "        time.sleep(2)\n",
    "        \n",
    "        img_tag_big = driver.find_elements_by_xpath('//img[@class=\"n3VNCb\"]')\n",
    "        #print('img_tag_big count: ', len(img_tag_big))\n",
    "        \n",
    "        index=0\n",
    "        for m in img_tag_big:\n",
    "            img_link = m.get_attribute('src')\n",
    "            if img_link[:4]=='data':\n",
    "                index+=1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        if img_link[:4]=='data':\n",
    "            print('---- Image link not found skipping ----')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(img_link, timeout=10, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                count+=1\n",
    "                print(\"count: \"+ str(count) + \"  index: \" + str(index) + \"  link: \" + img_link)\n",
    "                print('    Downloading and saving image')\n",
    "                file1 = open(r\"C:\\Temp\\images\\image_new_\" + search_input.replace(' ','_') + str(count) + '.jpg', \"wb\")\n",
    "                file1.write(response.content)\n",
    "                file1.flush()\n",
    "                file1.close()\n",
    "            else:\n",
    "                print('    Reponse status code: ', str(response.status_code))\n",
    "            response.close()\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            print('    Request timed out')\n",
    "\n",
    "        except Exception as ex:\n",
    "            template = \"    request.get(): An exception of type {0} occurred.\" # Arguments:\\n{1!r}\"\n",
    "            message = template.format(type(ex).__name__)  #, ex.args)\n",
    "            print(message)\n",
    "        else:\n",
    "            print('    File downloaded')    \n",
    "    print('Done')\n",
    "    return\n",
    "\n",
    "\n",
    "#Download_Images('fruits',10)\n",
    "#Download_Images('cars', 10)\n",
    "Download_Images('Machine Learning', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944393b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817a6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2857d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57502706",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://cars.usnews.com/images/article/202012/128775/1_2021_bugatti_chiron_super_sport.jpg'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"en-US,en;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "try:\n",
    "    response = requests.get(url, timeout=10, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print('Downloading and saving image')\n",
    "        file1 = open(r\"C:\\Temp\\images\\image_special_\" +\"55\"+ '.jpg', \"wb\")\n",
    "        file1.write(response.content)\n",
    "        file1.flush()\n",
    "        file1.close()\n",
    "    else:\n",
    "        print('    Reponse status code: ', str(response.status_code))\n",
    "    response.close()\n",
    "except requests.exceptions.ReadTimeout:\n",
    "    print('Request timed out')\n",
    "else:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffff2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83695d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e67a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working OK but uses direct search link for start\n",
    "def Download_Images(search_input):\n",
    "    search_url = f\"https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&q={search_input}\"\n",
    "    driver.get(search_url)\n",
    "    time.sleep(2)\n",
    "    img_tags = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "    \n",
    "    count=0\n",
    "    for img_tag in img_tags:\n",
    "        count+=1\n",
    "        if count>7:\n",
    "            break;\n",
    "        print(count)\n",
    "        img_tag.click()\n",
    "        delay =10\n",
    "        WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME,'n3VNCb')))\n",
    "    \n",
    "        img_tag_big = driver.find_elements_by_xpath('//img[@class=\"n3VNCb\"]')\n",
    "        print('img_tag_big count: ', len(img_tag_big))\n",
    "        \n",
    "        index=0\n",
    "        for m in img_tag_big:\n",
    "            img_link = m.get_attribute('src')\n",
    "            if img_link[:4]=='data':\n",
    "                index+=1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        if img_link[:4]=='data':\n",
    "            print('Skipping count: ', count)\n",
    "            continue\n",
    "        \n",
    "        print(\"index: \" + str(index) + \" link: \" + img_link)\n",
    "    \n",
    "        response = requests.get(img_link)\n",
    "        file1 = open(r\"C:\\Temp\\images\\image_new_\" + search_input.replace(' ','_') + str(count) + '.jpg', \"wb\")\n",
    "        file1.write(response.content)\n",
    "        file1.close()\n",
    "        \n",
    "    print('Done')\n",
    "    return\n",
    "    \n",
    "# #         #base64.b64decode(a)\n",
    "        \n",
    "# #         with open(\"C:\\\\temp\\\\images\\\\imageToSave_\"+ str(count)+\".jpg\", \"wb\") as fh:\n",
    "# #             #fh.write(link_img)\n",
    "# #             fh.write(base64.b64decode(link_img))\n",
    "# #             #fh.write(base64.decodebytes(link_img))\n",
    "\n",
    "Download_Images('fruits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192a13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ab08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working small images using decode base64#\n",
    "import base64\n",
    "\n",
    "def Download_Images(search_input):\n",
    "    driver.get('https://images.google.com')\n",
    "    time.sleep(2)\n",
    "    srch_bar = driver.find_element_by_xpath('//input')\n",
    "    srch_bar.clear\n",
    "    btn = driver.find_elements_by_xpath('//button')[1]\n",
    "    srch_bar.send_keys(search_input)\n",
    "    btn.click()\n",
    "    \n",
    "    delay = 5\n",
    "    WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME,'rg_i')))\n",
    "    \n",
    "    #'//a[@class=\"wXeWr islib nfEiy\"]'\n",
    "    img_tags = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "    print(len(img_tags))\n",
    "    \n",
    "    count = 0\n",
    "    for img_tag in img_tags:\n",
    "        count+=1\n",
    "        if count >2:\n",
    "            break\n",
    "        \n",
    "        print(count)\n",
    "        \n",
    "        link_img = img_tag.get_attribute('src')[23:]\n",
    "        #print(link_img)\n",
    "        #base64.b64decode(a)\n",
    "        \n",
    "        with open(\"C:\\\\temp\\\\images\\\\imageToSave_\"+ str(count)+\".jpg\", \"wb\") as fh:\n",
    "            #fh.write(link_img)\n",
    "            fh.write(base64.b64decode(link_img))\n",
    "            #fh.write(base64.decodebytes(link_img))\n",
    "        \n",
    "        \n",
    "        #print('img_tag.get_attribute(\"src\"): ', link_img)  # img_tag.get_attribute('src'))\n",
    "        \n",
    "#         #driver.get(img_tag.get_attribute('href'))\n",
    "#         #time.sleep(2)\n",
    "        \n",
    "        \n",
    "#         #link_img = driver.find_element_by_xpath('//a[@class=\"eHAdSb\"]/img').get_attribute('src')\n",
    "#         #print('link_img: ', link_img)\n",
    "        \n",
    "#         # print(img_tag.get_attribute('src'))\n",
    "        \n",
    "#         #'//a[@class=\"eHAdSb\"]/img' # in the target page get src attribute\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "#         response = requests.get('http://' + link_img)\n",
    "#         file1 = open(r\"C:\\Temp\\images\\image_new_\" + search_input.replace(' ','_') + str(count+1) +'.jpg', \"wb\")\n",
    "#         file1.write(response.content)\n",
    "#         file1.close()\n",
    "    return\n",
    "\n",
    "Download_Images('fruits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02282a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b22b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b73e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_wanted = ['fruits','cars','Machine Learning','Guitar','Cakes']\n",
    "\n",
    "#for i in range(1):\n",
    "if True:\n",
    "    driver.get('https://images.google.com')\n",
    "    time.sleep(2)\n",
    "    srch_bar = driver.find_element_by_xpath('//input')\n",
    "    srch_bar.clear\n",
    "    btn = driver.find_elements_by_xpath('//button')[1]\n",
    "    srch_bar.send_keys(image_wanted[0])\n",
    "    btn.click()\n",
    "#     time.sleep(2)\n",
    "#     #delay = 10                 # seconds\n",
    "#     #WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME,'rg_i Q4LuWd')))\n",
    "    \n",
    "#     img_tags= driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "#     for j in range(10):\n",
    "#         print(str(j) + img_tags[j].get_attribute('src'))\n",
    "# #         response=requests.get(img_tags[j].get_attribute('src'))\n",
    "# #         file1 = open(r\"C:\\Temp\\images\\image_\" + image_wanted[i].replace(' ','_') + str(j) +'.jpg', \"wb\")\n",
    "# #         file1.write(response.content)\n",
    "# #         file1.close()\n",
    "# #         file1.flush()\n",
    "#         break\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tags= driver.find_elements_by_xpath('//div[@class=\"bRMDJf islir\"]/img')\n",
    "print(img_tags[0].get_attribute('src'))\n",
    "# for i in range(10):\n",
    "#     response=requests.get(img_tags[i].get_attribute('src'))\n",
    "#     file1 = open(r\"C:\\Temp\\images\\image_\" + str(i) +'.jpg', \"wb\")\n",
    "#     file1.write(response.content)\n",
    "#     file1.close()\n",
    "#     break\n",
    "# print('Done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67460f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     response=requests.get(img_urls[i])\n",
    "#     file = open(r\"C:\\Temp\\images\\\"+str(i)+\".jpg\", \"wb\")\n",
    "#     file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270fa31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working : Full images #\n",
    "\n",
    "from selenium import webdriver\n",
    "import time, requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def search_google(search_query):\n",
    "    browser = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")\n",
    "    search_url = f\"https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&q={search_query}\"\n",
    "    images_url = []\n",
    "\n",
    "    # open browser and begin search\n",
    "    browser.get(search_url)\n",
    "    elements = browser.find_elements_by_class_name('rg_i')\n",
    "\n",
    "    count = 0\n",
    "    for e in elements:\n",
    "        # get images source url\n",
    "        e.click()\n",
    "        time.sleep(1)\n",
    "        element = browser.find_elements_by_class_name('v4dQwb')\n",
    "\n",
    "        # Google image web site logic\n",
    "        if count == 0:\n",
    "            big_img = element[0].find_element_by_class_name('n3VNCb')\n",
    "        else:\n",
    "            big_img = element[1].find_element_by_class_name('n3VNCb')\n",
    "\n",
    "        images_url.append(big_img.get_attribute(\"src\"))\n",
    "\n",
    "        # write image to file\n",
    "        response = requests.get(images_url[count])\n",
    "        if response.status_code == 200:\n",
    "            file1 = open(r\"C:\\Temp\\images\\image_\" + search_query.replace(' ', '_') + str(count+1) +'.jpg', \"wb\")\n",
    "            file1.write(response.content)\n",
    "            #file1.flush()\n",
    "            file1.close()\n",
    "\n",
    "#             with open(f\"C:\\\\temp\\\\images\\\\search_{count+1}.jpg\",\"wb\") as file:\n",
    "#                 file.write(reponse.content)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        # Stop get and save after 5\n",
    "        if count == 5:\n",
    "            break\n",
    "\n",
    "    return images_url\n",
    "\n",
    "items = search_google('fruits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9a5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f56e1444",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q 4. Write  a  python  program  to  search  for  a  smartphone(e.g.:  Oneplus  Nord,  pixel  4A,  etc.)  on \n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1 st  page. Details to be scraped:<br> “Brand  Name”, “Smartphone  name”,  “Colour”, “RAM”,  “Storage(ROM)”,  “Primary  Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. <br>Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_product_on_flipkart(search_product):\n",
    "    product = []\n",
    "    brand = []\n",
    "    color = []\n",
    "    ram = []\n",
    "    rom = []\n",
    "    camera = []\n",
    "    camera2 = []\n",
    "    display = []\n",
    "    battery = []\n",
    "    price = []\n",
    "    purl = []\n",
    "    \n",
    "    # Close popup asking for login\n",
    "    try:\n",
    "        driver.get('https://www.flipkart.com')\n",
    "        close_btn = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]')\n",
    "        close_btn.click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    search_input = driver.find_element_by_xpath('//input[@class=\"_3704LK\"]')\n",
    "    search_input.send_keys(search_product)\n",
    "    time.sleep(2)\n",
    "    search_btn = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')\n",
    "    search_btn.click()\n",
    "    time.sleep(2)\n",
    "    p_tags = driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "\n",
    "#     count=0\n",
    "    for p_tag in p_tags:\n",
    "#         count+=1\n",
    "#         if count > 3:\n",
    "#             break;\n",
    "        \n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        p_tag.click()                \n",
    "        time.sleep(2)\n",
    "        driver.switch_to.window(driver.window_handles[-1])  # switch to latest tab\n",
    "        #print(len(driver.window_handles))\n",
    "        purl.append(driver.current_url)\n",
    "\n",
    "        try:\n",
    "            product_name = driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]').text\n",
    "            product.append(product_name)\n",
    "            brand.append(product_name.split(' ')[0])\n",
    "        except:\n",
    "            product.append('-')\n",
    "            brand.append('-')\n",
    "\n",
    "        try:\n",
    "            color.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV _3AsE0T\"]/div[1]/table/tbody/tr[4]/td[2]/ul/li').text)\n",
    "            #'//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[8]/div[5]/div/div[2]/div[1]/div[4]/table/tbody/tr[2]/td[2]/ul/li'\n",
    "        except:\n",
    "            color.append('-')\n",
    "\n",
    "        try:\n",
    "            price.append(driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]').text.replace('₹',''))\n",
    "        except:\n",
    "            price.append('-')\n",
    "\n",
    "\n",
    "\n",
    "        btn_n = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')\n",
    "        btn_n.click()    \n",
    "        time.sleep(0.5)\n",
    "\n",
    "        try:\n",
    "            ram.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[4]/table/tbody/tr[2]/td[2]/ul/li').text)\n",
    "        except:\n",
    "            ram.append('-')\n",
    "\n",
    "\n",
    "        try:\n",
    "            rom.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[4]/table/tbody/tr[1]/td[2]/ul/li').text)\n",
    "        except:\n",
    "            rom.append('-')        \n",
    "\n",
    "        try:\n",
    "            camera.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[5]/table/tbody/tr[2]/td[2]/ul/li').text)\n",
    "        except:\n",
    "            camera.append('-')\n",
    "\n",
    "        try:\n",
    "            camera2.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[5]/table/tbody/tr[6]/td[2]/ul/li').text)\n",
    "        except:\n",
    "            camera2.append('-')\n",
    "    #     while len(driver.window_handles)>1:\n",
    "    #         driver.close()\n",
    "        try:\n",
    "            display.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[2]/table/tbody/tr[1]/td[2]/ul/li').text)\n",
    "        except:\n",
    "            display.append('-')\n",
    "\n",
    "        try:\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[9]/table/tbody/tr/td[2]/ul/li').text[0:1].isnumeric():\n",
    "                battery.append(driver.find_element_by_xpath('//div[@class=\"_1UhVsV\"]/div[9]/table/tbody/tr/td[2]/ul/li').text)\n",
    "            else:\n",
    "                battery.append('--')\n",
    "        except:\n",
    "            battery.append('-')\n",
    "\n",
    "        #break;\n",
    "\n",
    "    #driver.close()    \n",
    "    df = pd.DataFrame()\n",
    "    df['Brand']= brand\n",
    "    df['Product']= product\n",
    "    df['Color']= color\n",
    "    df['RAM']= ram\n",
    "    df['ROM']= rom\n",
    "    df['Primary Camera'] = camera\n",
    "    df['Secondary Camera']= camera2\n",
    "    df['Battery']= battery\n",
    "    df['Price']= price\n",
    "    df['Url'] = purl\n",
    "\n",
    "    df.to_csv(r'C:\\temp\\flipkart_mobiles.csv', index = False)\n",
    "\n",
    "    \n",
    "find_product_on_flipkart('pixel 4A')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a99e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25f8967f",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q 5. Write  a  program  to  scrap  geospatial  coordinates  (latitude,  longitude)  of  a  city  searched  on  google maps. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://maps.google.com/')\n",
    "search_box = driver.find_element_by_xpath('//input[@class=\"tactile-searchbox-input\"][1]')\n",
    "search_box.send_keys('Delhi')\n",
    "btn = driver.find_element_by_xpath('//button[@id=\"searchbox-searchbutton\"]')\n",
    "btn.click()\n",
    "time.sleep(4)\n",
    "\n",
    "url = driver.current_url;\n",
    "\n",
    "index = url[url.find('@')+1:].split(',')\n",
    "print(\"Latitude: \" + index[0] + \"  Longitude: \"+ index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1dc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38bcb97d",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) \n",
    "from trak.in</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://trak.in/')\n",
    "btn = driver.find_element_by_xpath('//*[@id=\"menu-item-51510\"]/a')\n",
    "driver.get(btn.get_attribute('href'))\n",
    "#driver.find_element_by_xpath('//table[@class=\"tablepress tablepress-id-54 dataTable no-footer\"]')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_heads = driver.find_elements_by_xpath('//*[@id=\"tablepress-57_wrapper\"]/div[3]/div[1]/div/table/thead/tr/th')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunction(p_xpath):\n",
    "    dates = []\n",
    "    comps = []\n",
    "    industries = []\n",
    "    subvertical = []\n",
    "    city = []\n",
    "    investor = []\n",
    "    investtype = []\n",
    "    amount = []\n",
    "    rows = driver.find_elements_by_xpath(p_xpath)    # ('//*[@id=\"tablepress-54\"]/tbody/tr')\n",
    "    for row in rows:\n",
    "        dates.append(row.find_elements_by_tag_name('td')[1].text)\n",
    "        comps.append(row.find_elements_by_tag_name('td')[2].text)\n",
    "        industries.append(row.find_elements_by_tag_name('td')[3].text)\n",
    "        subvertical.append(row.find_elements_by_tag_name('td')[4].text)\n",
    "        city.append(row.find_elements_by_tag_name('td')[5].text)\n",
    "        investor.append(row.find_elements_by_tag_name('td')[6].text)\n",
    "        investtype.append(row.find_elements_by_tag_name('td')[7].text)\n",
    "        amount.append(row.find_elements_by_tag_name('td')[8].text)\n",
    "    df = pd.DataFrame()\n",
    "    df['Date'] = dates\n",
    "    df[\"Startup\"] = comps\n",
    "    df['Industry'] = industries\n",
    "    df['SubVertical'] = subvertical\n",
    "    df['City'] = city\n",
    "    df['Investor'] = investor\n",
    "    df['Investment type'] = investtype\n",
    "    df['Amount USD'] = amount\n",
    "    #df.to_csv(r\"C:\\temp\\fin.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = myfunction('//*[@id=\"tablepress-54\"]/tbody/tr')\n",
    "df2 = myfunction('//*[@id=\"tablepress-55\"]/tbody/tr')\n",
    "df3 = myfunction('//*[@id=\"tablepress-56\"]/tbody/tr')\n",
    "\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "\n",
    "df.to_csv(r'C:\\temp\\fin1.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc13de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e39a52f",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q 7. Write a program to scrap all the available details of best gaming laptops from digit.in.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1246584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00451a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the page\n",
    "driver.get('https://digit.in/')\n",
    "time.sleep(2)\n",
    "\n",
    "# Go to the page Laptops\n",
    "driver.get(driver.find_element_by_xpath('/html/body/div[2]/div/div[4]/ul/li[3]/a').get_attribute('href'))\n",
    "time.sleep(2)\n",
    "\n",
    "# Extract gaming laptops link\n",
    "ul = driver.find_element_by_xpath('//ul[@class=\"list-unstyled sidebar-list\"]')\n",
    "for el in ul.find_elements_by_tag_name('li'):\n",
    "    if(el.text.lower().__contains__('gaming')):\n",
    "        gaming_link = el.find_element_by_tag_name('a').get_attribute('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0010a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lap_links(gaming_laptops_link):\n",
    "    links = []\n",
    "    if len(gaming_laptops_link) > 0:\n",
    "        driver.get(gaming_laptops_link)\n",
    "        time.sleep(2)\n",
    "        for lap in driver.find_elements_by_xpath('//div[@class=\"right-container\"]/div[@class=\"TopNumbeHeading active sticky-footer\"]/a'): # ('//div[@class=\"right-container\"]/div[1]/a'):\n",
    "            links.append(lap.get_attribute('href'))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laptop_details(lap_links):\n",
    "    product = []\n",
    "    os = []\n",
    "    display = []\n",
    "    processor = []\n",
    "    speed = []\n",
    "    core = []\n",
    "    memory = []\n",
    "    ramtype = []\n",
    "    model = []\n",
    "    weight = []\n",
    "    dimension = []\n",
    "    gprocessor = []\n",
    "    stype = []\n",
    "    scapacity = []\n",
    "    sstype = []\n",
    "    sscapacity = []\n",
    "    pbackup = []\n",
    "    btype = []\n",
    "    pwattage = []\n",
    "    speaker = []\n",
    "    url = []\n",
    "    count =0\n",
    "    for lnk in lap_links:\n",
    "        count+=1\n",
    "        driver.get(lnk)\n",
    "        time.sleep(2)\n",
    "\n",
    "        url.append(driver.current_url)\n",
    "        try:\n",
    "            product.append(driver.find_element_by_xpath('/html/body/div[4]/div[2]/div[2]/h1').text)\n",
    "        except:\n",
    "            product.append('-')\n",
    "\n",
    "        try:\n",
    "            os.append(driver.find_element_by_xpath('//div[@class=\"specs_details \"]/ul/li[1]/div/p[2]').text)\n",
    "        except:\n",
    "            os.append('-')\n",
    "\n",
    "        try:\n",
    "            display.append(driver.find_element_by_xpath('//div[@class=\"specs_details \"]/ul/li[2]/div/p[2]').text)\n",
    "            #processor.append(driver.find_element_by_xpath('//div[@class=\"specs_details \"]/ul/li[3]/div/p[2]').text)\n",
    "        except:\n",
    "            display.append('-')\n",
    "\n",
    "        try:\n",
    "            processor.append(driver.find_element_by_xpath('//*[@id=\"specs-processor\"]/div[2]/table/tbody/tr[1]/td[3]').text)\n",
    "        except:\n",
    "            processor.append('-')\n",
    "\n",
    "        try:\n",
    "            speed.append(driver.find_element_by_xpath('//*[@id=\"specs-processor\"]/div[2]/table/tbody/tr[2]/td[3]').text)\n",
    "        except:\n",
    "            speed.append('-')\n",
    "\n",
    "        try:\n",
    "            core.append(driver.find_element_by_xpath('//*[@id=\"specs-processor\"]/div[2]/table/tbody/tr[3]/td[3]').text)\n",
    "        except:\n",
    "            core.append('-')\n",
    "\n",
    "        try:\n",
    "            gprocessor.append(driver.find_element_by_xpath('//*[@id=\"specs-processor\"]/div[2]/table/tbody/tr[4]/td[3]').text)\n",
    "            #memory.append(driver.find_element_by_xpath('//div[@class=\"specs_details \"]/ul/li[4]/div/p[2]').text)\n",
    "        except:\n",
    "            gprocessor.append('-')\n",
    "\n",
    "        try:\n",
    "            model.append(driver.find_element_by_xpath('//*[@id=\"specs-basic-information\"]/div[2]/table/tbody/tr[1]/td[3]').text)\n",
    "        except:\n",
    "            model.append('-')\n",
    "\n",
    "        try:\n",
    "            memory.append(driver.find_element_by_xpath('//*[@id=\"specs-memory\"]/div[2]/table/tbody/tr[1]/td[3]').text)\n",
    "        except:\n",
    "            memory.append('-')\n",
    "\n",
    "        try:\n",
    "            ramtype.append(driver.find_element_by_xpath('//*[@id=\"specs-memory\"]/div[2]/table/tbody/tr[2]/td[3]').text)\n",
    "        except:\n",
    "            ramtype.append('-')\n",
    "\n",
    "        try:\n",
    "            stype.append(driver.find_element_by_xpath('//*[@id=\"specs-storage\"]/div[2]/table/tbody/tr[1]/td[3]').text)\n",
    "        except:\n",
    "            stype.append('-')\n",
    "\n",
    "        try:\n",
    "            scapacity.append(driver.find_element_by_xpath('//*[@id=\"specs-storage\"]/div[2]/table/tbody/tr[2]/td[3]').text)\n",
    "        except:\n",
    "            scapacity.append('-')\n",
    "\n",
    "        try:\n",
    "            sstype.append(driver.find_element_by_xpath('//*[@id=\"specs-storage\"]/div[2]/table/tbody/tr[3]/td[3]').text)\n",
    "        except:\n",
    "            sstype.append('-')\n",
    "\n",
    "        try:\n",
    "            sscapacity.append(driver.find_element_by_xpath('//*[@id=\"specs-storage\"]/div[2]/table/tbody/tr[4]/td[3]').text)\n",
    "        except:\n",
    "            sscapacity.append('-')\n",
    "\n",
    "        try:\n",
    "            weight.append(driver.find_element_by_xpath('//*[@id=\"specs-physical-specifications\"]/div[2]/table/tbody/tr[1]/td[3]').text)\n",
    "        except:\n",
    "            weight.append('-')\n",
    "\n",
    "        try:\n",
    "            dimension.append(driver.find_element_by_xpath('//*[@id=\"specs-physical-specifications\"]/div[2]/table/tbody/tr[2]/td[3]').text)\n",
    "        except:\n",
    "            dimension.append('-')\n",
    "\n",
    "        if(len(driver.find_elements_by_xpath('//*[@id=\"specs-power\"]/div[2]/table/tbody/tr[1]/td[1]'))>0):   # .text.lower().__contains__('battery backup')):\n",
    "\n",
    "            if driver.find_element_by_xpath('//*[@id=\"specs-power\"]/div[2]/table/tbody/tr[1]/td[1]').text.lower().__contains__('battery backup'):\n",
    "                cindex = 1\n",
    "            else:\n",
    "                cindex = 0\n",
    "            try:\n",
    "                pbackup.append(driver.find_element_by_xpath('//*[@id=\"specs-power\"]/div[2]/table/tbody/tr['+ str(cindex) +']/td[3]').text)\n",
    "            except:\n",
    "                pbackup.append('-')\n",
    "\n",
    "            try:\n",
    "                btype.append(driver.find_element_by_xpath('//*[@id=\"specs-power\"]/div[2]/table/tbody/tr['+ str(cindex+1)+']/td[3]').text)\n",
    "            except:\n",
    "                btype.append('-')\n",
    "            try:\n",
    "                pwattage.append(driver.find_element_by_xpath('//*[@id=\"specs-power\"]/div[2]/table/tbody/tr['+str(cindex+2)+']/td[3]').text)\n",
    "            except:\n",
    "                pwattage.append('-')\n",
    "        else:\n",
    "            pbackup.append('--')\n",
    "            btype.append('--')\n",
    "            pwattage.append('--')\n",
    "\n",
    "        try:\n",
    "            speaker.append(driver.find_element_by_xpath('//*[@id=\"specs-sound\"]/div[2]/table/tbody/tr/td[3]').text)\n",
    "        except:\n",
    "            speaker.append('-')\n",
    "\n",
    "        print(count)\n",
    "        #break\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['URL'] = url\n",
    "    df[\"Product\"] = product\n",
    "    df['Model'] = model\n",
    "    df['OS'] = os\n",
    "    df['Display'] = display\n",
    "    df['Processor'] = processor\n",
    "    df['Speed'] = speed\n",
    "    df['Core'] = core\n",
    "    df['Memory'] = memory\n",
    "    df['RamType'] = ramtype\n",
    "    df['Graphics Processor']=gprocessor\n",
    "    df['Weight(kg)'] = weight\n",
    "    df['Dimension'] = dimension\n",
    "    df['Storage Type'] = stype\n",
    "    df['Storage Capacity']=scapacity\n",
    "    df['2nd Storage Type'] = sstype\n",
    "    df['2nd Storage Capacity'] =sscapacity\n",
    "    df['Power Backup'] = pbackup\n",
    "    df['Battery Type'] =btype\n",
    "    df['Power'] = pwattage\n",
    "    df['Speaker'] =speaker\n",
    "    \n",
    "    return df\n",
    "\n",
    "links = get_lap_links(gaming_link)\n",
    "if len(links) > 0:\n",
    "    df = get_laptop_details(links)\n",
    "    df.to_csv(r'C:\\temp\\GamingLaptops.csv', index=False)\n",
    "    print('Done')\n",
    "else:\n",
    "    print('Not Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed074a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9bf472b",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be \n",
    "scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d59cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.forbes.com/billionaires/')\n",
    "time.sleep(3)\n",
    "rank = []\n",
    "name = []\n",
    "networth=[]\n",
    "age = []\n",
    "citizenship=[]\n",
    "source=[]\n",
    "industry=[]\n",
    "\n",
    "rank.append(driver.find_element_by_xpath('//div[@class=\"table-row expanded\"]/div[@class=\"rank\"]').text.replace('.',''))\n",
    "name.append(driver.find_element_by_xpath('//div[@class=\"table-row expanded\"]/div[@class=\"personName\"]').text)\n",
    "networth.append(driver.find_element_by_xpath('//div[@class=\"table-row expanded\"]/div[@class=\"netWorth\"]').text)\n",
    "age.append(driver.find_element_by_xpath('//div[@class=\"table-row expanded\"]/div[@class=\"age\"]').text)\n",
    "citizenship.append(driver.find_element_by_xpath('//div[@class=\"table-row expanded\"]/div[@class=\"countryOfCitizenship\"]').text)\n",
    "source.append(driver.find_element_by_xpath('//div[@class=\"table-row expanded\"]/div/div[@class=\"source-column\"]').text)\n",
    "industry.append(driver.find_element_by_xpath('//div[@class=\"table-row expanded\"]/div[@class=\"category\"]').text)\n",
    "\n",
    "\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"table-row \"]/div[@class=\"rank\"]'):\n",
    "    rank.append(i.text.replace('.',''))\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"table-row \"]/div[@class=\"personName\"]'):\n",
    "    name.append(i.text)\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"table-row \"]/div[@class=\"netWorth\"]'):\n",
    "    networth.append(i.text)\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"table-row \"]/div[@class=\"age\"]'):\n",
    "    age.append(i.text)\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"table-row \"]/div[@class=\"countryOfCitizenship\"]'):\n",
    "    citizenship.append(i.text)\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"table-row \"]/div/div[@class=\"source-column\"]'):\n",
    "    source.append(i.text)\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"table-row \"]/div[@class=\"category\"]'):\n",
    "    industry.append(i.text)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Rank'] = rank\n",
    "df['Name'] = name\n",
    "df['NetWorth']= networth\n",
    "df['Age'] = age\n",
    "df['CitizenShip']= citizenship\n",
    "df['Source']=source\n",
    "df['Industry']=industry\n",
    "\n",
    "df.to_csv(r\"C:\\temp\\billionaires.csv\", index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14132cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dd1b1b7",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd19041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dabd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.youtube.com/watch?v=Fz4ZMLsPzqM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529373f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(maxcount):\n",
    "    author = []\n",
    "    comment=[]\n",
    "    commenttime = []\n",
    "    upvote = []\n",
    "    comment_tags = driver.find_elements_by_xpath('//*[@id=\"contents\"]/ytd-comment-thread-renderer')\n",
    "    count=0\n",
    "    for i in comment_tags:\n",
    "        count+=1\n",
    "        if count > maxcount:\n",
    "            break\n",
    "            \n",
    "        if len(i.find_elements_by_xpath('.//*[@id=\"author-text\"]')[0].text) > 0:\n",
    "            author.append(i.find_elements_by_xpath('.//*[@id=\"author-text\"]')[0].text)\n",
    "        else:\n",
    "            author.append(i.find_elements_by_xpath('.//yt-formatted-string[@class=\"style-scope ytd-channel-name\"]')[0].text)\n",
    "            \n",
    "        comment.append(i.find_elements_by_xpath('.//yt-formatted-string[@class=\"style-scope ytd-comment-renderer\"][2]')[0].text)\n",
    "        \n",
    "        commenttime.append(i.find_elements_by_xpath('.//a[@class=\"yt-simple-endpoint style-scope yt-formatted-string\"]')[0].text)\n",
    "        \n",
    "        if i.find_elements_by_xpath('.//div[@class=\"style-scope ytd-comment-action-buttons-renderer\"]')[0].text.replace('\\n',' ').split(' ')[0][0].isnumeric():\n",
    "            upvote.append(i.find_elements_by_xpath('.//div[@class=\"style-scope ytd-comment-action-buttons-renderer\"]')[0].text.replace('\\n',' ').split(' ')[0])\n",
    "        else:\n",
    "            upvote.append('-')\n",
    "            \n",
    "    df = pd.DataFrame()\n",
    "    df['Name']=author\n",
    "    df['Comment']=comment\n",
    "    df['Time']= commenttime\n",
    "    df['Upvote']=upvote\n",
    "    return df\n",
    "    #'//*[@id=\"contents\"]/ytd-comment-thread-renderer[1]'\n",
    "\n",
    "\n",
    "# l_ele = driver.find_element_by_xpath(\"(//*[@id='content'])[last()]\")\n",
    "# # scroll to the last comment currently loaded\n",
    "# l_ele.location_once_scrolled_into_view\n",
    "# # wait until the comments loading is done\n",
    "\n",
    "#driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# scroll to the bottom in order to load the comments\n",
    "# driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "def increase_pagesize(min_count):\n",
    "    count=0\n",
    "    loopcount=0\n",
    "    while count < min_count:\n",
    "        \n",
    "        # Extra check to stop the loop\n",
    "        loopcount+=1\n",
    "        if loopcount>30:\n",
    "            break\n",
    "\n",
    "        html = driver.find_element_by_tag_name('html')\n",
    "        html.send_keys(Keys.END)\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        count = len(driver.find_elements_by_xpath('//*[@id=\"content\"]'))\n",
    "        print(count)\n",
    "    #print('Comments available:',count)\n",
    "    \n",
    "comments = 500  \n",
    "increase_pagesize(comments)\n",
    "df = get_comments(comments)\n",
    "df.to_csv(r'C:\\temp\\comments.csv', index=False)\n",
    "print('Done')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a5459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb36a5fc",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:orange;\">Q 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abbeb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad034f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\temp\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572ab5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.hostelworld.com/hostels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1f21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the hostels 1st page\n",
    "search_box = driver.find_element_by_xpath('//*[@id=\"home-search-keywords\"]')\n",
    "search_box.send_keys('London')\n",
    "time.sleep(2)\n",
    "\n",
    "search_box.send_keys(Keys.ENTER)\n",
    "time.sleep(.5)\n",
    "srch_btn =  driver.find_element_by_xpath('//*[@id=\"top-search\"]/div/div[2]/button')\n",
    "srch_btn.click()\n",
    "time.sleep(5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e45c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function, Returns all the details including link, except Facility and Description\n",
    "def get_df_with_hostel_details():\n",
    "    prop_links = []\n",
    "    hname = []\n",
    "    distance=[]\n",
    "    ratings=[]\n",
    "    total_reviews=[]\n",
    "    overall_review=[]\n",
    "    private_price=[]\n",
    "    dorm_price= []\n",
    "    facility=[]\n",
    "    description=[]\n",
    "    for i in driver.find_elements_by_xpath('//div[@id=\"__layout\"]/div/div/div[4]/div/div/div[@class=\"property-card\"]'):\n",
    "        facility.append('-')\n",
    "        description.append('-')\n",
    "        j = i.find_elements_by_xpath('./div[2]/div[1]/h2/a')\n",
    "        if len(j) > 0:\n",
    "            prop_links.append(j[0].get_attribute('href'))\n",
    "            hname.append(j[0].text)\n",
    "        else:\n",
    "            prop_links.append('-')\n",
    "            hname.append('-')\n",
    "\n",
    "        j = i.find_elements_by_xpath('./div[2]/div[1]/div[1]/a/span[1]')\n",
    "        if len(j) > 0:\n",
    "            distance.append(j[0].text)\n",
    "        else:\n",
    "            distance.append('-')\n",
    "\n",
    "        j = i.find_elements_by_xpath('./div[2]/div[2]/a/div/div[1]')\n",
    "        if len(j) > 0:\n",
    "            ratings.append(j[0].text)\n",
    "        else:\n",
    "            ratings.append('-')\n",
    "\n",
    "        j = i.find_elements_by_xpath('./div[2]/div[2]/a/div/div[2]/div[2]')\n",
    "        if len(j) > 0:\n",
    "            total_reviews.append(j[0].text.replace('Total Reviews',''))\n",
    "        else:\n",
    "            total_reviews.append('-')\n",
    "\n",
    "        j = i.find_elements_by_xpath('./div[2]/div[2]/a/div/div[2]/div[1]')\n",
    "        if len(j) > 0:\n",
    "            overall_review.append(j[0].text.replace('Total Reviews',''))\n",
    "        else:\n",
    "            overall_review.append('-')\n",
    "\n",
    "        j = i.find_elements_by_xpath('./div[3]/a[1]/div[1]/div')\n",
    "        if len(j) > 0:\n",
    "            if len(j[0].text.strip().split(' ')) > 1:\n",
    "                private_price.append(j[0].text.strip().split(' ')[1])\n",
    "            else:\n",
    "                private_price.append(j[0].text.strip().split(' ')[0])\n",
    "        else:\n",
    "            private_price.append('-')\n",
    "\n",
    "        j = i.find_elements_by_xpath('./div[3]/a[1]/div[2]/div')\n",
    "        if len(j) > 0:\n",
    "            if len(j[0].text.strip().split(' ')) > 1:\n",
    "                dorm_price.append(j[0].text.strip().split(' ')[1])\n",
    "            else:\n",
    "                dorm_price.append(j[0].text.strip().split(' ')[0])\n",
    "        else:\n",
    "            dorm_price.append('-')\n",
    "    print(len(prop_links), len(hname), len(distance), len(ratings), len(total_reviews), len(overall_review), len(private_price))\n",
    "    df= pd.DataFrame()\n",
    "    df['Link']= prop_links\n",
    "    df['Name']=hname\n",
    "    df['Distance']=distance\n",
    "    df['Ratings']=ratings\n",
    "    df['Total Reviews']= total_reviews\n",
    "    df['Overall Reviews']= overall_review\n",
    "    df['Private Price']=private_price\n",
    "    df['Dorm Price']=dorm_price\n",
    "    df['Facility']= facility\n",
    "    df['Description']=description\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97948cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function, fills description, and facility\n",
    "def fill_facility_description(dff):\n",
    "    print(\"Extracting description and facilities\")\n",
    "    lcount=0\n",
    "    hcount=0\n",
    "    while lcount < len(dff):\n",
    "        hcount+=1\n",
    "        print(\"\\r   Hostel No: \"+str(hcount), end=\"\")\n",
    "        #if hcount%10==0:\n",
    "        #    print(\"  \", hcount)\n",
    "        driver.get(dff.iloc[lcount]['Link'])\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            dff.loc[lcount]['Description'] = driver.find_element_by_xpath('//div[@class=\"description-container\"]/div/div[2]').text\n",
    "        except:\n",
    "            dff.loc[lcount]['Description'] = '--'\n",
    "        try:\n",
    "            dff.loc[lcount]['Facility']=driver.find_element_by_xpath('//*[@id=\"facilities-section\"]/div/ul').text #.replace('\\n',', ')\n",
    "        except:\n",
    "            dff.loc[lcount]['Facility']='--'\n",
    "        lcount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f0d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting details from page no: 1\n",
      "30 30 30 30 30 30 30\n",
      "Extracting details from page no: 2\n",
      "30 30 30 30 30 30 30\n",
      "Extracting details from page no: 3\n",
      "16 16 16 16 16 16 16\n",
      "Extracting description and facilities\n",
      "   Hostel No: 76No of hostels extracted: 76\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# loop through all pages and fill hostels details and store result in a dataframe\n",
    "df1 = pd.DataFrame()\n",
    "page_count=0\n",
    "while True:\n",
    "    page_count+=1\n",
    "    print('Extracting details from page no:', page_count)\n",
    "    df1 = df1.append(get_df_with_hostel_details(), ignore_index=True)\n",
    "    # Goto next page\n",
    "    try:\n",
    "        btn_next = driver.find_element_by_xpath('//div[@class=\"pagination-item pagination-next\"]')\n",
    "    except:\n",
    "        break\n",
    "    btn_next.click()\n",
    "    time.sleep(3)\n",
    "fill_facility_description(df1)    \n",
    "df1.to_csv(r'C:\\Temp\\hostels_final.csv')\n",
    "print('No of hostels extracted:',len(df1))    \n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3a659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b731d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5a9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23db629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bde043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc45107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95def5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1d25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6da62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a6c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c892132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
